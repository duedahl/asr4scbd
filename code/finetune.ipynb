{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1960feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os, sys, argparse, json\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import wandb\n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default config\n"
     ]
    }
   ],
   "source": [
    "def load_config(config_path: Optional[str] = None) -> DictConfig:\n",
    "    \"\"\"Load config from YAML file or use defaults.\"\"\"\n",
    "    \n",
    "    # Default configuration\n",
    "    default_config = OmegaConf.create({\n",
    "        \"TUNE_HYPERPARAMS\": True,\n",
    "        \"INITIAL_WEIGHTS\": None,\n",
    "        \"DATASET\": \"rvi_mdb\", \n",
    "        \"BASE_PATH\": \".\", # Set to directory containing datasets etc\n",
    "        \"EXPERIMENT_NAME\": \"tuned\",\n",
    "        \"MAX_TRACE_LENGTH\": 1000,\n",
    "        \"MAX_TRACE_EXPANSION\": 5,\n",
    "        \"FIXED_HYPERPARAMS\": {\n",
    "            \"num_train_steps\": 1000,\n",
    "            \"group_by_length\": False,\n",
    "            \"eval_steps\": 10,\n",
    "            \"logging_steps\": 10,\n",
    "        },\n",
    "        \"DEFAULT_TUNABLE_HYPERPARAMS\": {\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"per_device_train_batch_size\": 32,\n",
    "            \"weight_decay\": 0.005,\n",
    "            \"warmup_steps\": 100,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"freeze_feature_encoder\": True,\n",
    "        },\n",
    "        \"SAMPLING_RATE\": 200e6,\n",
    "    })\n",
    "    \n",
    "    # Check command line if no path provided\n",
    "    if config_path is None:\n",
    "        config_path = OmegaConf.from_cli().get(\"config\")\n",
    "    \n",
    "    # Load and merge YAML config\n",
    "    if config_path and os.path.exists(config_path):\n",
    "        yaml_config = OmegaConf.load(config_path)\n",
    "        config = OmegaConf.merge(default_config, yaml_config)\n",
    "        print(f\"Loaded config: {config_path}\")\n",
    "    else:\n",
    "        config = default_config\n",
    "        print(\"Using default config\")\n",
    "    return config\n",
    "\n",
    "# Simple one-liner setup\n",
    "def setup_config(config_path: Optional[str] = None):\n",
    "    \"\"\"Load config and inject into calling module's globals.\"\"\"\n",
    "    config = load_config(config_path)\n",
    "    sys._getframe(1).f_globals.update(config)\n",
    "\n",
    "setup_config()\n",
    "\n",
    "DATASET_PATH = os.path.join(BASE_PATH, f\"datasets/{DATASET}\")\n",
    "MODEL_PATH = os.path.join(BASE_PATH, \"runs\")\n",
    "EXPERIMENT_IDENTIFIER = f\"{EXPERIMENT_NAME}-{DATASET}\"\n",
    "TARGET_PATH = os.path.join(MODEL_PATH,EXPERIMENT_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d875b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning: ENABLED\n",
      "Using default pretrained weights\n",
      "Optuna imported successfully for hyperparameter tuning\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hyperparameter tuning: {'ENABLED' if TUNE_HYPERPARAMS else 'DISABLED'}\")\n",
    "if INITIAL_WEIGHTS:\n",
    "    print(f\"Using initial weights from: {INITIAL_WEIGHTS}\")\n",
    "else:\n",
    "    print(\"Using default pretrained weights\")\n",
    "\n",
    "# [Your existing vocabulary and data setup code remains the same]\n",
    "# ... (vocabulary creation, tokenizer setup, dataset loading, etc.)\n",
    "\n",
    "# Import hyperparameter tuning libraries only if needed\n",
    "if TUNE_HYPERPARAMS:\n",
    "    try:\n",
    "        import optuna\n",
    "        print(\"Optuna imported successfully for hyperparameter tuning\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: Optuna not installed. Install with: pip install optuna\")\n",
    "        TUNE_HYPERPARAMS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49afa99",
   "metadata": {},
   "source": [
    "## Set up key objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vocabulary\n",
    "path_vocab = os.path.join(DATASET_PATH, \"tokenizer.vocab\")\n",
    "with open(path_vocab, \"r\") as f:\n",
    "    data = f.readlines()\n",
    "vocab_dict = {key.strip(): val for val, key in enumerate(data)}\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "vocab_dict[\" \"] = len(vocab_dict) # Delimiter, is it needed?\n",
    "\n",
    "path_vocab = os.path.join(DATASET_PATH,'vocab.json')\n",
    "with open(path_vocab, 'w') as f:\n",
    "    json.dump(vocab_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4112a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up key model parts\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(path_vocab, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\" \")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=SAMPLING_RATE, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "print(f\"Length of tokenizer: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c66b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dataset_with_repetitions(dataset, shuffle=True):\n",
    "    \"\"\"\n",
    "    Expand each matrix entry into N separate dataset entries, each with the same labels\n",
    "    \"\"\"\n",
    "    expanded_data = {\n",
    "        \"input_values\": [],\n",
    "        \"input_length\": [],\n",
    "        \"labels\": [],\n",
    "        \"program_label\": []  # Keep original text labels for reference\n",
    "    }\n",
    "    \n",
    "    for example in dataset:\n",
    "        # Load numpy trace data\n",
    "        traces = np.load(example[\"traces\"])\n",
    "\n",
    "        if traces.shape[1] > MAX_TRACE_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        # Handle both single traces and matrices\n",
    "        if traces.ndim == 1:\n",
    "            # Single trace - keep as one entry\n",
    "            traces = [traces]\n",
    "        else:\n",
    "            # Matrix - each row becomes a separate entry\n",
    "            traces = [traces[j] for j in range(traces.shape[0]) if j < MAX_TRACE_EXPANSION]\n",
    "        \n",
    "        # Process labels once for this example\n",
    "        processed_labels = processor(text=example[\"program_label\"]).input_ids\n",
    "        \n",
    "        # Create separate dataset entries for each trace repetition\n",
    "        for trace in traces:\n",
    "            # Process the trace\n",
    "            processed_trace = processor(trace, sampling_rate=SAMPLING_RATE).input_values[0]\n",
    "            \n",
    "            # Add to expanded dataset\n",
    "            expanded_data[\"input_values\"].append(processed_trace)\n",
    "            expanded_data[\"input_length\"].append(len(processed_trace))\n",
    "            expanded_data[\"labels\"].append(processed_labels)\n",
    "            expanded_data[\"program_label\"].append(example[\"program_label\"])\n",
    "        \n",
    "    if shuffle:\n",
    "        # Convert to list of indices and shuffle\n",
    "        indices = list(range(len(expanded_data[\"input_values\"])))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Reorder all lists according to shuffled indices\n",
    "        for key in expanded_data:\n",
    "            expanded_data[key] = [expanded_data[key][i] for i in indices]\n",
    "    \n",
    "    return expanded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea13145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "data_paths = [os.path.join(DATASET_PATH, f\"{split}.json\") for split in splits]\n",
    "data = load_dataset(\"json\", data_files={key:val for key, val in zip([\"train\",\"validation\",\"test\"],data_paths)}, field=\"batches\")\n",
    "\n",
    "print(\"Expanding datasets with repetitions...\")\n",
    "\n",
    "# Expand each split\n",
    "expanded_datasets = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    if split in data:\n",
    "        print(f\"Expanding {split} split...\")\n",
    "        expanded_data = expand_dataset_with_repetitions(data[split])\n",
    "        expanded_datasets[split] = Dataset.from_dict(expanded_data)\n",
    "        print(f\"  {split}: {len(data[split])} -> {len(expanded_datasets[split])} examples\")\n",
    "\n",
    "# Create the final dataset\n",
    "from datasets import DatasetDict\n",
    "dataset = DatasetDict(expanded_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug(pred_str, pred_logits, pred_ids, pred, label_str):\n",
    "    # === DEBUGGING SECTION ===\n",
    "    print(\"\\n=== COMPUTE_METRICS DEBUG ===\")\n",
    "    print(f\"Batch size: {len(pred_str)}\")\n",
    "    print(f\"Pred logits shape: {pred_logits.shape}\")\n",
    "    print(f\"Pred ids shape: {pred_ids.shape}\")\n",
    "    print(f\"Label ids shape: {pred.label_ids.shape}\")\n",
    "    \n",
    "    import random\n",
    "    sample_indices = random.sample(range(len(pred_str)), 3)\n",
    "\n",
    "    # Check raw predictions and labels\n",
    "    print(f\"\\nRaw pred_ids (first 3 samples, first 10 tokens):\")\n",
    "    for i in sample_indices:\n",
    "        print(f\"  Sample {i}: {pred_ids[i][:10]}\")\n",
    "    \n",
    "    print(f\"\\nRaw label_ids (first 3 samples, first 10 tokens):\")\n",
    "    for i in sample_indices:\n",
    "        print(f\"  Sample {i}: {pred.label_ids[i][:10]}\")\n",
    "    \n",
    "    # Check decoded strings\n",
    "    print(f\"\\nDecoded predictions (first 3):\")\n",
    "    for i in sample_indices:\n",
    "        print(f\"  Pred {i}: '{pred_str[i]}'\")\n",
    "    \n",
    "    print(f\"\\nDecoded labels (first 3):\")\n",
    "    for i in sample_indices:\n",
    "        print(f\"  Label {i}: '{label_str[i]}'\")\n",
    "    \n",
    "    # Check vocabulary usage\n",
    "    flat_pred_ids = pred_ids.flatten()\n",
    "    flat_label_ids = pred.label_ids.flatten()\n",
    "    flat_label_ids = flat_label_ids[flat_label_ids != -100]  # Remove padding\n",
    "    \n",
    "    print(f\"\\nVocabulary analysis:\")\n",
    "    print(f\"  Processor vocab size: {processor.tokenizer.vocab_size}\")\n",
    "    print(f\"  Unique pred tokens used: {len(np.unique(flat_pred_ids))}\")\n",
    "    print(f\"  Pred token range: {flat_pred_ids.min()}-{flat_pred_ids.max()}\")\n",
    "    print(f\"  Unique label tokens: {len(np.unique(flat_label_ids))}\")\n",
    "    print(f\"  Label token range: {flat_label_ids.min()}-{flat_label_ids.max()}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    if len(set(pred_str)) == 1:\n",
    "        print(f\"  WARNING: All predictions are identical: '{pred_str[0]}'\")\n",
    "    \n",
    "    if len(set(label_str)) <= 5:\n",
    "        print(f\"  INFO: Only {len(set(label_str))} unique labels in batch: {set(label_str)}\")\n",
    "    \n",
    "    # Check for empty predictions/labels\n",
    "    empty_preds = sum(1 for p in pred_str if not p.strip())\n",
    "    empty_labels = sum(1 for l in label_str if not l.strip())\n",
    "    print(f\"  Empty predictions: {empty_preds}/{len(pred_str)}\")\n",
    "    print(f\"  Empty labels: {empty_labels}/{len(label_str)}\")\n",
    "    \n",
    "    # Sample token-by-token comparison\n",
    "    print(f\"\\nSample comparisons (first 3):\")\n",
    "    for i in range(min(3, len(pred_str))):\n",
    "        print(f\"  Sample {i}:\")\n",
    "        print(f\"    Pred: '{pred_str[i]}'\")\n",
    "        print(f\"    Label: '{label_str[i]}'\")\n",
    "        print(f\"    Match: {pred_str[i] == label_str[i]}\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "\n",
    "def compute_metrics(pred, verbose=True):\n",
    "    import evaluate\n",
    "\n",
    "    # Load the WER metric\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, spaces_between_special_tokens=True)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False, spaces_between_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    if verbose:\n",
    "        print_debug(pred_str, pred_logits, pred_ids, pred, label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1598aa",
   "metadata": {},
   "source": [
    "## Set up training and tuning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5da1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'else' statement on line 75 (1286412964.py, line 78)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel = create_model(hyperparams)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'else' statement on line 75\n"
     ]
    }
   ],
   "source": [
    "def get_model_path():\n",
    "    \"\"\"Get the model path to use for initialization\"\"\"\n",
    "    if INITIAL_WEIGHTS and os.path.exists(INITIAL_WEIGHTS):\n",
    "        return INITIAL_WEIGHTS\n",
    "    else:\n",
    "        default_path = os.path.join(MODEL_PATH, f\"wav2vec2-pretrain-{DATASET}\")\n",
    "        if not os.path.exists(default_path) and INITIAL_WEIGHTS:\n",
    "            print(f\"Warning: Specified initial weights path {INITIAL_WEIGHTS} not found, using default\")\n",
    "        return default_path\n",
    "\n",
    "def create_model(hyperparams):\n",
    "    \"\"\"Create and configure model based on hyperparameters\"\"\"\n",
    "    from transformers import Wav2Vec2ForCTC\n",
    "    \n",
    "    model_path = get_model_path()\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        model_path,\n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    if hyperparams.get(\"freeze_feature_encoder\", True):\n",
    "        model.freeze_feature_encoder()\n",
    "        print(\"Feature encoder frozen\")\n",
    "    else:\n",
    "        print(\"Feature encoder unfrozen\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_training_args(hyperparams):\n",
    "    \"\"\"Create training arguments from hyperparameters\"\"\"\n",
    "    from transformers import TrainingArguments\n",
    "    \n",
    "    # Combine tunable and fixed hyperparameters\n",
    "    all_hyperparams = {**FIXED_HYPERPARAMS, **hyperparams}\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=TARGET_PATH,\n",
    "        learning_rate=all_hyperparams[\"learning_rate\"],\n",
    "        per_device_train_batch_size=all_hyperparams[\"per_device_train_batch_size\"],\n",
    "        weight_decay=all_hyperparams[\"weight_decay\"],\n",
    "        warmup_steps=all_hyperparams[\"warmup_steps\"],\n",
    "        gradient_accumulation_steps=all_hyperparams[\"gradient_accumulation_steps\"],\n",
    "        max_steps=all_hyperparams[\"num_train_steps\"],\n",
    "        group_by_length=all_hyperparams[\"group_by_length\"],\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=all_hyperparams[\"eval_steps\"],\n",
    "        logging_steps=all_hyperparams[\"logging_steps\"],\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        save_steps=all_hyperparams[\"eval_steps\"] if not TUNE_HYPERPARAMS else 10e6, # De-Facto - don't save\n",
    "        save_total_limit=1 if not TUNE_HYPERPARAMS else 0,\n",
    "        load_best_model_at_end=not TUNE_HYPERPARAMS,\n",
    "        metric_for_best_model=\"eval_wer\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[\"wandb\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(hyperparams, verbose=True):\n",
    "    \"\"\"Train model with given hyperparameters\"\"\"\n",
    "    from transformers import Trainer\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=EXPERIMENT_IDENTIFIER,\n",
    "        name=timestamp,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create model and training args\n",
    "        model = create_model(hyperparams)\n",
    "        training_args = create_training_args(hyperparams)\n",
    "\n",
    "        # Select dataset (subset for tuning, full for normal training)\n",
    "        train_data = dataset[\"train\"]\n",
    "        eval_data = dataset[\"validation\"]\n",
    "\n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            data_collator=data_collator,\n",
    "            args=training_args,\n",
    "            compute_metrics=lambda pred: compute_metrics(pred, verbose=verbose),\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=eval_data,\n",
    "            processing_class=processor.feature_extractor,\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate BEFORE finishing wandb session\n",
    "        eval_result = trainer.evaluate()\n",
    "\n",
    "        result = {\n",
    "            \"wer\": eval_result[\"eval_wer\"],\n",
    "            \"eval_loss\": eval_result[\"eval_loss\"],\n",
    "            \"trainer\": trainer,\n",
    "            \"hyperparams\": hyperparams\n",
    "        }\n",
    "\n",
    "        if not TUNE_HYPERPARAMS: # avoid multiprocessing issue for optuna\n",
    "            result[\"model\"] = model\n",
    "\n",
    "        return result\n",
    "\n",
    "    finally:\n",
    "        # Always finish wandb session, even if there's an exception\n",
    "        run.finish()\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    \"\"\"Perform hyperparameter tuning using Optuna\"\"\"\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    print(f\"Fixed parameters: {FIXED_HYPERPARAMS}\")\n",
    "\n",
    "    def objective(trial):\n",
    "        # Clear any CUDA cache at start of each trial\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Define search space (only tunable parameters)\n",
    "        hyperparams = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "            \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [32, 64, 128, 256]),\n",
    "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.001, 0.1),\n",
    "            \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 50, 500),\n",
    "            \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2]),\n",
    "            \"freeze_feature_encoder\": trial.suggest_categorical(\"freeze_feature_encoder\", [True, False]),\n",
    "        }\n",
    "\n",
    "        print(f\"Trial {trial.number}: {hyperparams}\")\n",
    "\n",
    "        # Train with subset\n",
    "        result = train_model(\n",
    "            hyperparams, \n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Return eval_loss instead of WER\n",
    "        print(f\"Trial {trial.number} eval_loss: {result['eval_loss']:.4f}, WER: {result['wer']:.4f}\")\n",
    "        return result[\"eval_loss\"]\n",
    "\n",
    "    # Create study - direction should be \"minimize\" for loss (lower is better)\n",
    "    \n",
    "    db_filename = \"optuna.db\"\n",
    "    db_path = os.path.join(TARGET_PATH, db_filename)\n",
    "    os.makedirs(TARGET_PATH, exist_ok=True)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=\"wav2vec2_tuning_loss\",\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    # Optimize\n",
    "    study.optimize(objective, n_trials=200, timeout=3600*12)  # 4 hours max\n",
    "\n",
    "    print(\"Hyperparameter tuning completed!\")\n",
    "    print(f\"Best eval_loss: {study.best_value:.4f}\")  # Changed message\n",
    "    print(\"Best parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    print(\"Fixed parameters (not tuned):\")\n",
    "    for key, value in FIXED_HYPERPARAMS.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Save best parameters (only tunable ones)\n",
    "    best_hyperparams = study.best_params.copy()\n",
    "\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TUNE_HYPERPARAMS:\n",
    "    # Run hyperparameter tuning\n",
    "    best_hyperparams = hyperparameter_tuning()\n",
    "    \n",
    "    print(f\"Finished Optuna run.\\n\")\n",
    "else:\n",
    "    # Normal training mode\n",
    "    print(\"Running normal training...\")\n",
    "    \n",
    "    print(\"Using default hyperparameters\")\n",
    "    hyperparams = DEFAULT_TUNABLE_HYPERPARAMS\n",
    "    \n",
    "    # Train model\n",
    "    result = train_model(hyperparams, verbose=True)\n",
    "    \n",
    "    # Save model\n",
    "    result[\"model\"].save_pretrained(TARGET_PATH)\n",
    "    processor.save_pretrained(TARGET_PATH)\n",
    "    print(f\"Model saved to {TARGET_PATH}\")\n",
    "    print(f\"Final WER: {result['wer']:.4f}\")\n",
    "    print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
