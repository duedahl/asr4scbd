{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbccef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import math\n",
    "import os, sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "from datetime import datetime\n",
    "\n",
    "import aiohttp\n",
    "import datasets\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    SchedulerType,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    get_scheduler,\n",
    "    is_wandb_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "from transformers.utils import send_example_telemetry\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
    "import wandb\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# BASED ON:\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f454076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretraining_config(config_path: Optional[str] = None) -> DictConfig:\n",
    "    \"\"\"Load pretraining config from YAML file or use defaults.\"\"\"\n",
    "    \n",
    "    # Default configuration matching your argparse setup\n",
    "    default_config = OmegaConf.create({\n",
    "        \"tune_hyperparams\": False,\n",
    "        \n",
    "        # Dataset arguments\n",
    "        \"base_path\": \".\", # Set to directory containing datasets etc\n",
    "        \"normalize_features\": True,\n",
    "        \"group_by_length\": False,\n",
    "        \"dataset_name\": \"pre_rvi_mdb\",\n",
    "        \"experiment_name\": \"pretrain_local\",\n",
    "        \"dataset_config_names\": [\"clean\", \"clean\"],  # Will be required, set in validation\n",
    "        \"dataset_split_names\": [\"train\", \"val\"],   # Will be required, set in validation\n",
    "        \"data_column_name\": \"traces\",\n",
    "        \n",
    "        # Preprocessing arguments\n",
    "        \"preprocessing_num_workers\": 4,\n",
    "        \"max_trace_expansion\":5,\n",
    "        \n",
    "        # Training arguments\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5, # OPTUNA\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"max_train_steps\": None, #overrides num_train_epochs\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"lr_scheduler_type\": \"linear\",\n",
    "        \"num_warmup_steps\": 100,\n",
    "        \"output_dir\": None,\n",
    "        \"seed\": 0,\n",
    "        \n",
    "        # Optimizer arguments\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.98,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \n",
    "        # Logging and saving\n",
    "        \"logging_steps\": 10,\n",
    "        \"saving_steps\": 100,\n",
    "        \n",
    "        # Hub arguments\n",
    "        \"push_to_hub\": False,\n",
    "        \"hub_model_id\": None,\n",
    "        \"hub_token\": None,\n",
    "        \n",
    "        # Trace-specific arguments\n",
    "        \"sampling_rate\": 200000000,  # 200*10**6\n",
    "        \"max_trace_length\": 10000,  # 10**6\n",
    "        \"min_trace_length\": 0,\n",
    "        \"pad_to_multiple_of\": None,\n",
    "        \n",
    "        # Gumbel softmax arguments\n",
    "        \"max_gumbel_temperature\": 2.0,\n",
    "        \"min_gumbel_temperature\": 0.5,\n",
    "        \"gumbel_temperature_decay\": 0.999995,\n",
    "        \n",
    "        # Masking arguments\n",
    "        \"mask_time_prob\": 0.5,\n",
    "        \"mask_time_length\": 4,\n",
    "\n",
    "        # Model configuration\n",
    "        \"model\": {\n",
    "            # Core architecture parameters\n",
    "            \"hidden_size\": 64,\n",
    "            \"num_hidden_layers\": 12, # OPTUNA, [6, 8, 12]\n",
    "            \"num_attention_heads\": 4, # OPTUNA, [2,4,8]\n",
    "            \n",
    "            # Feature extraction parameters\n",
    "            \"conv_dim\": [256, 128, 64],\n",
    "            \"conv_stride\": [4, 2, 1], # OPTUNA, [[4, 2, 2], [2, 2, 1]]\n",
    "            \"conv_kernel\": [8, 4, 3],\n",
    "            \n",
    "            # Normalization settings\n",
    "            \"feat_extract_norm\": \"layer\",\n",
    "            \"layer_norm_eps\": 1e-5,\n",
    "            \"do_stable_layer_norm\": True, # Test without normalization - maybe only the one in the feature extractor?\n",
    "            \n",
    "            # Quantization/codebook parameters\n",
    "            \"vocab_size\": 16, # FIXME: This is only relevant during finetuning, adapt in finetuning file.\n",
    "            \"num_codevectors_per_group\": 8, # OPTUNA, [6, 8, 10]\n",
    "            \"num_codevector_groups\": 2, # OPTUNA, [1, 2]\n",
    "            \"num_negatives\": 100,\n",
    "            \n",
    "            # Regularization\n",
    "            \"hidden_dropout\": 0.1,\n",
    "            \"attention_dropout\": 0.1,\n",
    "            \"activation_dropout\": 0.0,\n",
    "            \"feat_proj_dropout\": 0.0,\n",
    "            \"layerdrop\": 0.05,\n",
    "            \"diversity_loss_weight\": 1,\n",
    "            \n",
    "            # Activation function\n",
    "            \"hidden_act\": \"gelu\",\n",
    "            \n",
    "            # Other necessary parameters for pretraining\n",
    "            \"mask_feature_prob\": 0.0,\n",
    "            \"mask_feature_length\": 5,\n",
    "            \"ctc_loss_reduction\": \"sum\",\n",
    "            \"ctc_zero_infinity\": False,\n",
    "            \n",
    "            # Initialize with random weights\n",
    "            \"initializer_range\": 0.02,\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Check command line if no path provided\n",
    "    if config_path is None:\n",
    "        config_path = OmegaConf.from_cli().get(\"config\")\n",
    "    \n",
    "    # Load and merge YAML config\n",
    "    if config_path and os.path.exists(config_path):\n",
    "        yaml_config = OmegaConf.load(config_path)\n",
    "        config = OmegaConf.merge(default_config, yaml_config)\n",
    "        print(f\"Loaded pretraining config: {config_path}\")\n",
    "    else:\n",
    "        config = default_config\n",
    "        print(\"Using default pretraining config\")\n",
    "    \n",
    "    # Validation (equivalent to argparse required fields and assertions)    \n",
    "    if not config.dataset_config_names:\n",
    "        raise ValueError(\"dataset_config_names is required\")\n",
    "    \n",
    "    if not config.dataset_split_names:\n",
    "        raise ValueError(\"dataset_split_names is required\")\n",
    "    \n",
    "    # Create output directory if specified\n",
    "    if config.output_dir is not None:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def config_to_namespace(config: DictConfig) -> Namespace:\n",
    "    \"\"\"Convert OmegaConf config to argparse Namespace for compatibility.\"\"\"\n",
    "    return Namespace(**OmegaConf.to_container(config, resolve=True))\n",
    "\n",
    "def setup_pretraining_config(config_path: Optional[str] = None):\n",
    "    \"\"\"Load config and inject both config and args into calling module's globals.\"\"\"\n",
    "    config = load_pretraining_config(config_path)\n",
    "    args = config_to_namespace(config)\n",
    "    \n",
    "    # Inject both config and args into calling module's globals\n",
    "    caller_globals = sys._getframe(1).f_globals\n",
    "    caller_globals.update(config)\n",
    "    caller_globals['config'] = config\n",
    "    caller_globals['args'] = args\n",
    "    \n",
    "    return config, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5296276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default pretraining config\n"
     ]
    }
   ],
   "source": [
    "setup_pretraining_config()\n",
    "EXPERIMENT_IDENTIFIER = f\"{args.experiment_name}-{args.dataset_name}\"\n",
    "if not args.output_dir:\n",
    "    args.output_dir = os.path.join(args.base_path,\"runs\",EXPERIMENT_IDENTIFIER)\n",
    "\n",
    "args.per_device_eval_batch_size = args.batch_size\n",
    "args.per_device_train_batch_size = args.batch_size\n",
    "\n",
    "# Import hyperparameter tuning libraries only if needed\n",
    "if args.tune_hyperparams:\n",
    "    try:\n",
    "        import optuna\n",
    "        print(\"Optuna imported successfully for hyperparameter tuning\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: Optuna not installed. Install with: pip install optuna\")\n",
    "        args.tune_hyperparams = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9d20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForWav2Vec2Pretraining:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received and prepare masked indices\n",
    "    for self-supervised pretraining.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`~transformers.Wav2Vec2ForPreTraining`):\n",
    "            The Wav2Vec2 model used for pretraining. The data collator needs to have access\n",
    "            to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n",
    "        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n",
    "            The processor used for processing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        mask_time_prob (:obj:`float`, `optional`, defaults to :obj:`0.65`):\n",
    "            Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked for the contrastive task.\n",
    "            Note that overlap between masked sequences may decrease the actual percentage of masked vectors.\n",
    "            The default value is taken from the original wav2vec 2.0 article (https://arxiv.org/abs/2006.11477),\n",
    "            and results in about 49 percent of each sequence being masked on average.\n",
    "        mask_time_length (:obj:`int`, `optional`, defaults to :obj:`10`):\n",
    "            Length of each vector mask span to mask along the time axis in the contrastive task. The default value\n",
    "            originates from the original wav2vec 2.0 article and corresponds to the ``M`` variable mentioned there.\n",
    "    \"\"\"\n",
    "\n",
    "    model: Wav2Vec2ForPreTraining\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    mask_time_prob: Optional[float] = 0.65\n",
    "    mask_time_length: Optional[int] = 10\n",
    "\n",
    "    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.feature_extractor.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        device = batch[\"input_values\"].device\n",
    "        batch_size = batch[\"input_values\"].shape[0]\n",
    "\n",
    "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
    "        # make sure masked sequence length is a Python scalar\n",
    "        mask_indices_seq_length = int(mask_indices_seq_length)\n",
    "\n",
    "        # make sure that no loss is computed on padded inputs\n",
    "        if batch.get(\"attention_mask\") is not None:\n",
    "            # compute real output lengths according to convolution formula\n",
    "            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
    "                mask_indices_seq_length, batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        features_shape = (batch_size, mask_indices_seq_length)\n",
    "\n",
    "        # sample randomly masked indices\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            features_shape,\n",
    "            self.mask_time_prob,\n",
    "            self.mask_time_length,\n",
    "            attention_mask=batch.get(\"sub_attention_mask\"),\n",
    "        )\n",
    "\n",
    "        # sample negative indices\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape,\n",
    "            self.model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "        )\n",
    "        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
    "        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def multiply_grads(params, c):\n",
    "    \"\"\"Multiplies grads by a constant *c*.\"\"\"\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            if torch.is_tensor(c):\n",
    "                c = c.to(p.grad.device)\n",
    "            p.grad.data.mul_(c)\n",
    "\n",
    "\n",
    "def get_grad_norm(params, scale=1):\n",
    "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebc657",
   "metadata": {},
   "source": [
    "# Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29535e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_datasets(args):\n",
    "    \"\"\"Setup and process datasets.\"\"\"\n",
    "    # 1. Download and create train, validation dataset\n",
    "    raw_datasets = DatasetDict()\n",
    "    for dataset_config_name, split_name in zip(args.dataset_config_names, args.dataset_split_names):\n",
    "        # load dataset\n",
    "        path_data = os.path.join(args.base_path, \"datasets\", args.dataset_name, f\"{split_name}.json\")\n",
    "        data = load_dataset(\"json\", data_files=path_data, field=\"batches\")\n",
    "        raw_datasets[split_name] = data[\"train\"]\n",
    "\n",
    "    # Apply length filters\n",
    "    max_length = args.max_trace_length\n",
    "    min_length = args.min_trace_length\n",
    "\n",
    "    if min_length > 0:\n",
    "        before_count = len(raw_datasets[\"train\"])\n",
    "        raw_datasets = raw_datasets.filter(\n",
    "            lambda x: x[\"trace_length\"] > min_length,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "        )\n",
    "        after_count = len(raw_datasets[\"train\"])\n",
    "        print(f\"Min length filter: {before_count} -> {after_count} (removed {before_count - after_count})\")\n",
    "\n",
    "    if max_length is not None:\n",
    "        before_count = len(raw_datasets[\"train\"])\n",
    "        raw_datasets = raw_datasets.filter(\n",
    "            lambda x: x[\"trace_length\"] <= max_length,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "        )\n",
    "        after_count = len(raw_datasets[\"train\"])\n",
    "        print(f\"Max length filter: {before_count} -> {after_count} (removed {before_count - after_count})\")\n",
    "\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def setup_feature_extractor(args):\n",
    "    \"\"\"Setup feature extractor.\"\"\"\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1, \n",
    "        sampling_rate=args.sampling_rate, \n",
    "        padding_value=0.0, \n",
    "        do_normalize=args.normalize_features, \n",
    "        return_attention_mask=False\n",
    "    )\n",
    "\n",
    "    # # only normalized-inputs-training is supported\n",
    "    # if not feature_extractor.do_normalize:\n",
    "    #     raise ValueError(\n",
    "    #         \"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``\"\n",
    "    #     )\n",
    "    \n",
    "    return feature_extractor\n",
    "\n",
    "\n",
    "def expand_dataset_entries(dataset, args, feature_extractor):\n",
    "    \"\"\"Expand each matrix entry into N separate dataset entries\"\"\"\n",
    "    expanded_data = {\n",
    "        \"input_values\": [],\n",
    "        \"input_length\": []\n",
    "    }\n",
    "\n",
    "    max_length = args.max_trace_length\n",
    "\n",
    "    for example in dataset:\n",
    "        # Load numpy trace data\n",
    "        traces = np.load(example[args.data_column_name])\n",
    "\n",
    "        # Handle both single traces and matrices\n",
    "        if traces.ndim == 1:\n",
    "            # Single trace - keep as one entry\n",
    "            traces = [traces]\n",
    "        else:\n",
    "            # Matrix - each row becomes a separate entry\n",
    "            traces = [traces[i] for i in range(traces.shape[0])]\n",
    "\n",
    "        # Create separate dataset entries for each trace\n",
    "        for i, trace in enumerate(traces):\n",
    "            if i >= args.max_trace_expansion:\n",
    "                break\n",
    "            inputs = feature_extractor(\n",
    "                trace, \n",
    "                sampling_rate=args.sampling_rate,\n",
    "                max_length=max_length,\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            expanded_data[\"input_values\"].append(inputs.input_values[0])\n",
    "            expanded_data[\"input_length\"].append(len(inputs.input_values[0]))\n",
    "\n",
    "    return expanded_data\n",
    "\n",
    "\n",
    "def setup_vectorized_datasets(args, raw_datasets, feature_extractor, accelerator):\n",
    "    \"\"\"Setup vectorized datasets by expanding matrix files.\"\"\"\n",
    "    # load trace files\n",
    "    with accelerator.main_process_first():\n",
    "        print(\"Expanding matrix files into individual traces...\")\n",
    "\n",
    "        # Process train and validation separately\n",
    "        train_expanded = expand_dataset_entries(raw_datasets[\"train\"], args, feature_extractor)\n",
    "        val_expanded = expand_dataset_entries(raw_datasets[\"val\"], args, feature_extractor)\n",
    "\n",
    "        print(f\"Train: {len(raw_datasets['train'])} -> {len(train_expanded['input_values'])} examples\")\n",
    "        print(f\"Validation: {len(raw_datasets['val'])} -> {len(val_expanded['input_values'])} examples\")\n",
    "\n",
    "        # Create new datasets from the expanded data\n",
    "        vectorized_datasets = DatasetDict({\n",
    "            \"train\": datasets.Dataset.from_dict(train_expanded),\n",
    "            \"val\": datasets.Dataset.from_dict(val_expanded)\n",
    "        })\n",
    "\n",
    "        # Remove input_length column\n",
    "        vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")\n",
    "\n",
    "    return vectorized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d391f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(args):\n",
    "    \"\"\"Setup Wav2Vec2 model with configuration.\"\"\"\n",
    "    config = Wav2Vec2Config(\n",
    "        # Core architecture parameters\n",
    "        hidden_size=args.model[\"hidden_size\"],\n",
    "        num_hidden_layers=args.model[\"num_hidden_layers\"],\n",
    "        num_attention_heads=args.model[\"num_attention_heads\"],\n",
    "\n",
    "        # Feature extraction parameters\n",
    "        conv_dim=args.model[\"conv_dim\"],\n",
    "        conv_stride=args.model[\"conv_stride\"],\n",
    "        conv_kernel=args.model[\"conv_kernel\"],\n",
    "\n",
    "        # Normalization settings\n",
    "        feat_extract_norm=args.model[\"feat_extract_norm\"],\n",
    "        layer_norm_eps=args.model[\"layer_norm_eps\"],\n",
    "        do_stable_layer_norm=args.model[\"do_stable_layer_norm\"],\n",
    "\n",
    "        # Masking parameters for pretraining\n",
    "        mask_time_prob=args.mask_time_prob,\n",
    "        mask_time_length=args.mask_time_length,\n",
    "\n",
    "        # Quantization/codebook parameters\n",
    "        vocab_size=args.model[\"vocab_size\"],\n",
    "        num_codevectors_per_group=args.model[\"num_codevectors_per_group\"],\n",
    "        num_codevector_groups=args.model[\"num_codevector_groups\"],\n",
    "        num_negatives=args.model[\"num_negatives\"],\n",
    "\n",
    "        # Regularization\n",
    "        hidden_dropout=args.model[\"hidden_dropout\"],\n",
    "        attention_dropout=args.model[\"attention_dropout\"],\n",
    "        activation_dropout=args.model[\"activation_dropout\"],\n",
    "        feat_proj_dropout=args.model[\"feat_proj_dropout\"],\n",
    "        layerdrop=args.model[\"layerdrop\"],\n",
    "        diversity_loss_weight=args.model[\"diversity_loss_weight\"],\n",
    "\n",
    "        # Activation function\n",
    "        hidden_act=args.model[\"hidden_act\"],\n",
    "\n",
    "        # Other necessary parameters for pretraining\n",
    "        mask_feature_prob=args.model[\"mask_feature_prob\"], # FIXME: Unused?\n",
    "        mask_feature_length=args.model[\"mask_feature_length\"], # FIXME: Unused?\n",
    "        ctc_loss_reduction=args.model[\"ctc_loss_reduction\"],\n",
    "        ctc_zero_infinity=args.model[\"ctc_zero_infinity\"],\n",
    "\n",
    "        # Initialize with random weights\n",
    "        initializer_range=args.model[\"initializer_range\"],\n",
    "    )\n",
    "\n",
    "    # pretraining is only supported for \"newer\" stable layer norm architecture\n",
    "    if not config.do_stable_layer_norm or config.feat_extract_norm != \"layer\":\n",
    "        raise ValueError(\n",
    "            \"PreTraining is only supported for ``config.do_stable_layer_norm=True`` and\"\n",
    "            \" ``config.feat_extract_norm='layer'\"\n",
    "        )\n",
    "\n",
    "    # initialize random model\n",
    "    model = Wav2Vec2ForPreTraining(config)\n",
    "\n",
    "    # Activate gradient checkpointing if needed\n",
    "    if args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0968806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_collator(args, model, feature_extractor):\n",
    "    \"\"\"Setup data collator for pretraining.\"\"\"\n",
    "    mask_time_prob = model.config.mask_time_prob if args.mask_time_prob is None else args.mask_time_prob\n",
    "    mask_time_length = model.config.mask_time_length if args.mask_time_length is None else args.mask_time_length\n",
    "\n",
    "    data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor,\n",
    "        pad_to_multiple_of=args.pad_to_multiple_of,\n",
    "        mask_time_prob=mask_time_prob,\n",
    "        mask_time_length=mask_time_length,\n",
    "    )\n",
    "    \n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def setup_dataloaders(args, vectorized_datasets, data_collator):\n",
    "    \"\"\"Setup train and validation dataloaders.\"\"\"\n",
    "\n",
    "    print(\"Setting up dataloaders...\")\n",
    "\n",
    "    if args.group_by_length:\n",
    "        # Add groupings to prevent excessive padding and wasted compute\n",
    "        sampler = LengthGroupedSampler(\n",
    "            batch_size=args.batch_size,\n",
    "            dataset=vectorized_datasets[\"train\"],\n",
    "            lengths=[len(item[\"input_values\"]) for item in vectorized_datasets[\"train\"]],\n",
    "        )\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            vectorized_datasets[\"train\"],\n",
    "            sampler=sampler,\n",
    "            collate_fn=data_collator,\n",
    "            batch_size=args.batch_size,\n",
    "        )\n",
    "    else:\n",
    "        train_dataloader = DataLoader(\n",
    "            vectorized_datasets[\"train\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            batch_size=args.batch_size,\n",
    "        )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        vectorized_datasets[\"val\"], \n",
    "        collate_fn=data_collator, \n",
    "        batch_size=args.batch_size,\n",
    "    )\n",
    "\n",
    "    print(f\"Train dataloader size: {len(train_dataloader)} batches\")\n",
    "\n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "\n",
    "def setup_optimizer_and_scheduler(args, model, train_dataloader):\n",
    "    \"\"\"Setup optimizer and learning rate scheduler.\"\"\"\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(model.parameters()),\n",
    "        lr=args.learning_rate,\n",
    "        betas=[args.adam_beta1, args.adam_beta2],\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26818ab4",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f25e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_args_with_optuna_params(args, trial):\n",
    "    \"\"\"Update args with Optuna hyperparameter suggestions.\"\"\"\n",
    "\n",
    "    # args.batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 48])\n",
    "    # args.normalize_features = trial.suggest_categorical(\"normalize_features\", [True, False])\n",
    "    args.learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    args.mask_time_prob = trial.suggest_float(\"mask_time_prob\", 0.05, 0.4, log=True)\n",
    "    args.mask_time_length = trial.suggest_int(\"mask_time_length\", 1, 8)\n",
    "    \n",
    "    # Model architecture parameters\n",
    "    # args.model[\"num_attention_heads\"] = trial.suggest_categorical(\"num_attention_heads\", [2, 4, 8])\n",
    "    args.model[\"num_hidden_layers\"] = trial.suggest_categorical(\"num_hidden_layers\", [6, 12])\n",
    "    args.model[\"num_codevectors_per_group\"] = trial.suggest_categorical(\"num_codevectors_per_group\", [4, 6, 8])\n",
    "    args.model[\"num_codevector_groups\"] = trial.suggest_categorical(\"num_codevector_groups\", [1, 2])\n",
    "    args.model[\"diversity_loss_weight\"] = trial.suggest_float(\"diversity_loss_weight\", 0.5, 2.0)\n",
    "    \n",
    "    # Conv stride configuration\n",
    "    conv_stride_options = [[4, 2, 2], [2, 2, 1], [2, 1, 1]]\n",
    "    args.model[\"conv_stride\"] = trial.suggest_categorical(\"conv_stride\", conv_stride_options)\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c8087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args, accelerator, vectorized_datasets):\n",
    "    total_batch_size = args.batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(vectorized_datasets['train'])}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "    # Setup all components\n",
    "    feature_extractor = setup_feature_extractor(args)\n",
    "    model = setup_model(args)\n",
    "    data_collator = setup_data_collator(args, model, feature_extractor)\n",
    "    train_dataloader, eval_dataloader = setup_dataloaders(args, vectorized_datasets, data_collator)\n",
    "    optimizer, lr_scheduler = setup_optimizer_and_scheduler(args, model, train_dataloader)\n",
    "   \n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # set up weights and biases if available\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    if is_wandb_available():\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.output_dir.split(\"/\")[-1], \n",
    "            dir=args.output_dir,\n",
    "            name=timestamp,\n",
    "            )\n",
    "    \n",
    "    # Set up logger\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    completed_steps = 0\n",
    "    starting_epoch = 0\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # compute num of losses\n",
    "            num_losses = batch[\"mask_time_indices\"].sum()\n",
    "            sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
    "            sub_attention_mask = (\n",
    "                sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
    "            )\n",
    "            percent_masked = num_losses / sub_attention_mask.sum()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # divide loss by gradient accumulation steps since gradients\n",
    "            # are accumulated for multiple backward passes in PyTorch\n",
    "            loss = outputs.loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            # make sure that `num_losses` is summed for distributed training\n",
    "            # and average gradients over losses of all devices\n",
    "            if accelerator.state.num_processes > 1:\n",
    "                num_losses = accelerator.gather_for_metrics(num_losses).sum()\n",
    "                gradient_multiplier = accelerator.state.num_processes / num_losses\n",
    "                multiply_grads(model.module.parameters(), gradient_multiplier)\n",
    "            else:\n",
    "                multiply_grads(model.parameters(), 1 / num_losses)\n",
    "\n",
    "            # update step\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                # compute grad norm for monitoring\n",
    "                scale = (\n",
    "                    accelerator.scaler._scale.item()\n",
    "                    if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n",
    "                    else 1\n",
    "                )\n",
    "                if accelerator.state.num_processes > 1:\n",
    "                    grad_norm = get_grad_norm(model.module.parameters(), scale)\n",
    "                else:\n",
    "                    grad_norm = get_grad_norm(model.parameters(), scale)\n",
    "\n",
    "                # update parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if not accelerator.optimizer_step_was_skipped:\n",
    "                    lr_scheduler.step()\n",
    "                elif accelerator.is_local_main_process:\n",
    "                    progress_bar.write(\n",
    "                        f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n",
    "                    )\n",
    "\n",
    "                # update gumbel temperature\n",
    "                gumbel_temperature = max(\n",
    "                    args.max_gumbel_temperature * args.gumbel_temperature_decay**completed_steps,\n",
    "                    args.min_gumbel_temperature,\n",
    "                )\n",
    "                if hasattr(model, \"module\"):\n",
    "                    model.module.set_gumbel_temperature(gumbel_temperature)\n",
    "                else:\n",
    "                    model.set_gumbel_temperature(gumbel_temperature)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            # 6. Log all results\n",
    "            if (step + 1) % (args.gradient_accumulation_steps * args.logging_steps) == 0:\n",
    "                loss.detach()\n",
    "                outputs.contrastive_loss.detach()\n",
    "                outputs.diversity_loss.detach()\n",
    "\n",
    "                if accelerator.state.num_processes > 1:\n",
    "                    loss = accelerator.gather_for_metrics(loss).sum()\n",
    "                    outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()\n",
    "                    outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()\n",
    "                    percent_masked = accelerator.gather_for_metrics(percent_masked).sum()\n",
    "\n",
    "                train_logs = {\n",
    "                    \"loss\": (loss * args.gradient_accumulation_steps) / num_losses,\n",
    "                    \"constrast_loss\": outputs.contrastive_loss / num_losses,\n",
    "                    \"div_loss\": outputs.diversity_loss / num_losses,\n",
    "                    \"%_mask_idx\": percent_masked / accelerator.num_processes,\n",
    "                    \"perplexity\": outputs.codevector_perplexity,\n",
    "                    \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
    "                    \"temp\": torch.tensor(gumbel_temperature),\n",
    "                    \"grad_norm\": torch.tensor(grad_norm),\n",
    "                }\n",
    "                log_str = \"\"\n",
    "                for k, v in train_logs.items():\n",
    "                    log_str += f\"| {k}: {v.item():.3e}\"\n",
    "\n",
    "                if accelerator.is_local_main_process:\n",
    "                    progress_bar.write(log_str)\n",
    "                    if is_wandb_available():\n",
    "                        wandb.log(train_logs)\n",
    "\n",
    "            # if completed steps > `args.max_train_steps` stop\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "        \n",
    "        # 7. Validate!\n",
    "        model.eval()\n",
    "\n",
    "        # init logs\n",
    "        val_logs = {\n",
    "            \"val_loss\": 0,\n",
    "            \"val_contrastive_loss\": 0,\n",
    "            \"val_diversity_loss\": 0,\n",
    "            \"val_num_losses\": 0,\n",
    "        }\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                batch.pop(\"sub_attention_mask\", None)\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            val_logs[\"val_loss\"] += outputs.loss\n",
    "            val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
    "            val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
    "            val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
    "\n",
    "        # sum over devices in multi-processing\n",
    "        if accelerator.num_processes > 1:\n",
    "            val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n",
    "\n",
    "        val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
    "        val_loss_final = val_logs[\"val_loss\"].item()\n",
    "\n",
    "        log_str = \"\"\n",
    "        for k, v in val_logs.items():\n",
    "            log_str += f\"| {k}: {v.item():.3e}\"\n",
    "\n",
    "        if accelerator.is_local_main_process:\n",
    "            progress_bar.write(log_str)\n",
    "            if is_wandb_available():\n",
    "                wandb.log(val_logs)\n",
    "\n",
    "        if (args.output_dir is not None) and (not args.tune_hyperparams):\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "            )\n",
    "\n",
    "    return {\"val_loss\": val_loss_final}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23888a",
   "metadata": {},
   "source": [
    "# Hyperparam Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f47bb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(args, accelerator, vectorized_datasets):\n",
    "    \"\"\"Perform hyperparameter tuning using Optuna\"\"\"\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Clear CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create a copy of args to avoid modifying the original\n",
    "        trial_args = Namespace(**vars(args))\n",
    "\n",
    "        # Update search space directly in the args variable\n",
    "        trial_args = update_args_with_optuna_params(trial_args, trial)\n",
    "\n",
    "        # Train with trial parameters\n",
    "        try:\n",
    "            result = train_model(trial_args, accelerator, vectorized_datasets)\n",
    "            val_loss = result[\"val_loss\"]\n",
    "            print(f\"Trial {trial.number} completed with val_loss: {val_loss:.4f}\")\n",
    "            return val_loss\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed with error: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    # Create study\n",
    "    db_filename = \"optuna.db\"\n",
    "    db_path = os.path.join(args.output_dir, db_filename)\n",
    "    print(args.output_dir)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=\"wav2vec2_tuning_loss\",\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(objective, n_trials=200, timeout=3600*15)  # 15 hours max\n",
    "    \n",
    "    print(\"Hyperparameter tuning completed!\")\n",
    "    print(f\"Best val_loss: {study.best_value:.4f}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6af648",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e051a25",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/workspaces/asr4scbd/./datasets/pre_rvi_mdb/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     set_seed(args.seed)\n\u001b[32m     12\u001b[39m accelerator.wait_for_everyone()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m raw_datasets = \u001b[43msetup_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m feature_extractor = setup_feature_extractor(args)\n\u001b[32m     16\u001b[39m vectorized_datasets = setup_vectorized_datasets(args, raw_datasets, feature_extractor, accelerator)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36msetup_datasets\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset_config_name, split_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args.dataset_config_names, args.dataset_split_names):\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[32m      7\u001b[39m     path_data = os.path.join(args.base_path, \u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m, args.dataset_name, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     data = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatches\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     raw_datasets[split_name] = data[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Apply length filters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/load.py:1782\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1781\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1795\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/load.py:1497\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1474\u001b[39m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[32m   1475\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1476\u001b[39m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1488\u001b[39m \n\u001b[32m   1489\u001b[39m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[32m   1491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path.endswith(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/load.py:913\u001b[39m, in \u001b[36mPackagedDatasetModuleFactory.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    907\u001b[39m base_path = Path(\u001b[38;5;28mself\u001b[39m.data_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).expanduser().resolve().as_posix()\n\u001b[32m    908\u001b[39m patterns = (\n\u001b[32m    909\u001b[39m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m.data_files)\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config=\u001b[38;5;28mself\u001b[39m.download_config)\n\u001b[32m    912\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m data_files = \u001b[43mDataFilesDict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m module_path, \u001b[38;5;28mhash\u001b[39m = _PACKAGED_DATASETS_MODULES[\u001b[38;5;28mself\u001b[39m.name]\n\u001b[32m    921\u001b[39m builder_kwargs = {\n\u001b[32m    922\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata_files\u001b[39m\u001b[33m\"\u001b[39m: data_files,\n\u001b[32m    923\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    924\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/data_files.py:690\u001b[39m, in \u001b[36mDataFilesDict.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    685\u001b[39m out = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns.items():\n\u001b[32m    687\u001b[39m     out[key] = (\n\u001b[32m    688\u001b[39m         patterns_for_key\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m     )\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/data_files.py:583\u001b[39m, in \u001b[36mDataFilesList.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    582\u001b[39m         data_files.extend(\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m         )\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    591\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/data_files.py:384\u001b[39m, in \u001b[36mresolve_pattern\u001b[39m\u001b[34m(pattern, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    383\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Unable to find '/workspaces/asr4scbd/./datasets/pre_rvi_mdb/train.json'"
     ]
    }
   ],
   "source": [
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_wav2vec2_pretraining_no_trainer\", args)\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "raw_datasets = setup_datasets(args)\n",
    "feature_extractor = setup_feature_extractor(args)\n",
    "vectorized_datasets = setup_vectorized_datasets(args, raw_datasets, feature_extractor, accelerator)\n",
    "\n",
    "if args.tune_hyperparams:\n",
    "    # Perform hyperparameter tuning\n",
    "    best_params = hyperparameter_tuning(args, accelerator, vectorized_datasets)\n",
    "    print(\"Optuna tuning completed.\")\n",
    "else:\n",
    "    # Train the model (either with original params or best params from tuning)\n",
    "    result = train_model(args, accelerator, vectorized_datasets)\n",
    "    print(f\"Training completed with final val_loss: {result['val_loss']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
